ЛК 6-2024.
I. Перекрестная энтропия. Сглаживание меток. Двоичная перекрестная энтропия.
II. Сверточные слои.
III. Слой подвыборки по максимуму.
IV. Слой пакетной нормализации.
V. Примеры с padding = 'same' / 'valid'.
VI. Порождающие состязательные нейронные сети. GAN.
VII. cGAN.
VIII. Варианты GAN
IX. Функции определения сходства/различия изображений.
X. Метод Монте-Карло.
XI. Контрольные вопросы
XII. Задачи 11 и 12

I. Перекрестная энтропия. Сглаживание меток. Двоичная перекрестная энтропия
Перекрестная энтропия (Cross Entropy, CE) - мера того, насколько одно распределение похоже на другое.
Перекрестная энтропия в терминах информатики - ожидаемая длина сообщения в битах при использовании 
распределения Q вместо распределения P,
где P – истинное распределение,
Q - искусственно смоделированное распределение: 

H(𝑝,𝑞) = −∫ 𝑝(𝑥) 𝑙𝑜𝑔 𝑞(𝑥)𝑑𝑥 (𝑙𝑜𝑔 - натуральный логарифм).
	  𝑥
В терминах машинного обучения
P: 𝑦 = y_true - one-hot-представление метки;
Q: 𝑝 = y_pred - прогноз модели:

𝐶𝐸(𝑦, 𝑝) = −∑ 𝑦𝑖 𝑙𝑜𝑔 𝑝𝑖 (сумма берется от 1 до n = len(y_true) = len(y_pred)).

Пример.
import keras
import numpy as np
y_true = np.array([[0, 1, 0], [0, 0, 1]])
y_pred = np.array([[0.05, 0.9, 0.05], [0.1, 0.8, 0.1]])
cce = keras.losses.CategoricalCrossentropy() # reduction = 'sum'
print(cce(y_true, y_pred)) # 1.177
print(-(np.log(0.95) + np.log(0.1)) / 2) # 1.177

# Сглаживание меток 
cce = keras.losses.CategoricalCrossentropy(label_smoothing = 0.3)
y_true_s = np.array([[0.1, 0.8, 0.1], [0.1, 0.1, 0.8]])
# Вместо [0, 1, 0] имеем: [0.1, 0.8, 0.1]
# 0.1 = 0.3 / num_classes
# 0.8 = (1 - 0.3) + 0.3 / num_classes
c = -np.vdot(y_true_s, np.log(y_pred)) / 2
print(cce(y_true, y_pred)) # 1.389
print(c) # 1.389
Сглаживание меток уменьшает уверенность, с которой модель выбирает положительную метку.
Сглаживание меток - метод регуляризации, повышает способность модели к обобщению.

Двоичная перекрестная энтропия (Binary Cross Entropy, BCE).
В этом случае один класс рассматривается как истинный (положительные примеры), а остальные 
классы рассматриваются как фон (background), то есть как отрицательные примеры. 
Тогда BCE задаётся следующей формулой: 

𝐵𝐶𝐸(𝑦, 𝑝) = −(𝑦 𝑙𝑜𝑔 𝑝 + (1 − 𝑦) 𝑙𝑜𝑔(1 − 𝑝)).

C точки зрения математической статистики перекрестная энтропия - это логарифм оценки максимального правдоподобия: 

𝐿(𝑥) = ∏ 𝑝(𝑥)𝑦(𝑥),
	𝑥
𝑙(𝑥) = 𝑙𝑜𝑔 𝐿 = ∑ 𝑦(𝑥) 𝑙𝑜𝑔 𝑝(𝑥).
	      𝑥
Если 𝑦(𝑥) ∈ {0, 1}, то получим:

𝑙(𝑝, 𝑦) = 𝑦 𝑙𝑜𝑔 𝑝 + (1 − 𝑦) 𝑙𝑜𝑔(1 − 𝑝).

Переходя от задачи максимизации к задаче минимизации функции 𝑙(𝑥) получим: 

min 𝑙(𝑦, 𝑝) = −max 𝑙(𝑦,𝑝) = −(𝑦 𝑙𝑜𝑔 𝑝 + (1 − 𝑦) 𝑙𝑜𝑔(1 − 𝑝)) = 𝐵𝐶𝐸(𝑦, 𝑝)
 𝑝	      𝑝 

II. Сверточные слои
Свертка 1d, см. рис.: -5 = 3 * 1 + 7 * (-2) + 6.
Обучаются параметры фильтра (ядра фильтра).
Свертка 2d, см. рис. - работа свертки; 3 - ядро свертки.
12 = 3*0 + 3*1 + 2*2 + 0*2 + 0*2 + 1*0 + 3*0 + 1*1 + 2*2
Пример.
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
num_filters = 8
pd = 'same' # 'valid'
x = Conv2D(num_filters, kernel_size = 3, padding = pd, activation = 'relu')(x)
params = num_filters * (kernel_size^2 * n_channels + bias) = 8 * (9 * 3 + 1) = 224 # bias = 1, смещение
params = 16 * 9 * 8 = 1152 (bias = 0, use_bias = False)

III. Слой подвыборки  по максимуму
Снижает размерность модели.
См. рис.
x = MaxPooling2D(pool_size = 2, strides = 2, padding = 'same')

IV. Слой пакетной нормализации.

В случае НС на вход слоя подается тензор формы (размер_пакета, карта_признаков), то есть пакет признаков.
Поэтому нормализация признаков НС называется пакетной.
Пакетная нормализация выполняется по следующему алгоритму (случай одномерной карты признаков):
Входные данные: значения x из пакета B = {x1, x2, ..., xm} (xi – индивидуальный признак; m – размер пакета);
обучаемые параметры γ и β; константа ε для вычислительной устойчивости.
Выходные данные: {yi = BNγ, β(xi)}
См. рис.
Пакетная нормализация позволяет:
- уменьшить расхождение математических ожиданий и дисперсий признаков пакета;
- ускорить поиск решения, несмотря на выполнение дополнительных вычислений;
- каждому слою сети обучатся более независимо от других слоев;
- использовать более высокую скорость обучения (на выходах слоя НС нет чрезмерно больших или малых значений);
- осуществлять, в известной мере, регуляризацию, поскольку привносит в выходы слоя НС некоторый шум;
- сделать НС менее чувствительной к начальной инициализации весов.

V. Примеры с padding = 'same' / 'valid'

padding = 'same'
Model: "functional_1"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ input_layer (InputLayer)        │ (None, 32, 32, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization             │ (None, 32, 32, 3)      │            12 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d (Conv2D)                 │ (None, 32, 32, 8)      │           224 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d (MaxPooling2D)    │ (None, 16, 16, 8)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_1           │ (None, 16, 16, 8)      │            32 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (Conv2D)               │ (None, 16, 16, 16)     │         1,168 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_1 (MaxPooling2D)  │ (None, 8, 8, 16)       │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten (Flatten)               │ (None, 1024)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (Dropout)               │ (None, 1024)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 64)             │        65,600 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 10)             │           650 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 67,686 (264.40 KB)
 Trainable params: 67,664 (264.31 KB)
 Non-trainable params: 22 (88.00 B)

padding = 'valid'
Model: "functional_1"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ input_layer (InputLayer)        │ (None, 32, 32, 3)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization             │ (None, 32, 32, 3)      │            12 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d (Conv2D)                 │ (None, 30, 30, 8)      │           224 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d (MaxPooling2D)    │ (None, 15, 15, 8)      │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_1           │ (None, 15, 15, 8)      │            32 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv2d_1 (Conv2D)               │ (None, 13, 13, 16)     │         1,168 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ max_pooling2d_1 (MaxPooling2D)  │ (None, 6, 6, 16)       │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ flatten (Flatten)               │ (None, 576)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dropout (Dropout)               │ (None, 576)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 64)             │        36,928 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 10)             │           650 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 39,014 (152.40 KB)
 Trainable params: 38,992 (152.31 KB)
 Non-trainable params: 22 (88.00 B)

VI. Порождающие состязательные нейронные сети. GAN.

Порождающие состязательные нейронные сети (НС, Generative Adversarial Networks, GAN) обучаются на основе заданного набора генерировать из шума данные.
Например, на основе MNIST НС GAN можно обучить генерировать из шума изображения рукописных цифр.
Порождающая модель содержит две нейронные сети – генератор и дискриминатор.
Данные, понятно, порождает генератор; дискриминатор употребляется только при обучении НС:
- генератор обучается порождать объекты так, чтобы они были неотличимы от реальных примеров, например, изображений MNIST;
  генератор обучается в составе обобщенной модели, включающей и генератор, и дискриминатор;
- дискриминатор учится отличать порожденные генератором объекты от реальных примеров.
Реальные примеры берутся из обучающего множества (ОМ) набора данных.
В примере используется ОМ MNIST.
На вход генератора подается шум – массив noise формы (batch_size, latent_dim)
В приводимой ниже программе:
batch_size = 32
latent_dim = 100
Шум формируется, например, на основе нормального распределения:
import numpy as np
noise = np.random.normal(0, 1, (batch_size, latent_dim))
На выходе генератора тензор формы (batch_size, 28 * 28).
Цель дискриминатора – научиться надежно отличать порожденные генератором (поддельные) примеры от настоящих,
то есть дискриминатор решает задачу бинарной классификации:
по заданному примеру решить, это настоящий пример или пример, порожденный генератором.
Цель генератора – научиться порождать примеры с распределением pgen,
таким, чтобы дискриминатор не смог бы отличить pgen от распределения реальных данных pdata,
которые наряду с поддельными используются при обучении дискриминатора.
Таким образом, генератор пытается научиться порождать правильные примеры,
а дискриминатор – отличать порожденные примеры от настоящих.
По мере обучения генератор и дискриминатор, состязаясь, постепенно улучшают друг друга.
Обучение дискриминатора ведется на примерах двух видов – поддельных, порожденных генератором,
и настоящих, взятых из предварительно сформированного набора данных, например, MNIST.
Возможную схему обучения порождающей состязательной нейронной сети см. на рис.
Модель НС.
Model: "Generator"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ input_layer (InputLayer)        │ (None, 100)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization             │ (None, 100)            │           400 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense (Dense)                   │ (None, 256)            │        25,856 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu (LeakyReLU)         │ (None, 256)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_1           │ (None, 256)            │         1,024 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_1 (Dense)                 │ (None, 512)            │       131,584 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu_1 (LeakyReLU)       │ (None, 512)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ batch_normalization_2           │ (None, 512)            │         2,048 │
│ (BatchNormalization)            │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_2 (Dense)                 │ (None, 1024)           │       525,312 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu_2 (LeakyReLU)       │ (None, 1024)           │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_3 (Dense)                 │ (None, 784)            │       803,600 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 1,489,824 (5.68 MB)
 Trainable params: 1,488,088 (5.68 MB)
 Non-trainable params: 1,736 (6.78 KB)
Модель дискриминатора
Model: "Discriminator"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ input_layer_1 (InputLayer)      │ (None, 784)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_4 (Dense)                 │ (None, 512)            │       401,920 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu_3 (LeakyReLU)       │ (None, 512)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_5 (Dense)                 │ (None, 512)            │       262,656 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ leaky_re_lu_4 (LeakyReLU)       │ (None, 512)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ dense_6 (Dense)                 │ (None, 1)              │           513 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 665,089 (2.54 MB)
 Trainable params: 665,089 (2.54 MB)
 Non-trainable params: 0 (0.00 B)
Обобщенная модель
Model: "Combined"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ input_layer_2 (InputLayer)      │ (None, 100)            │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Generator (Functional)          │ (None, 784)            │     1,489,824 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ Discriminator (Functional)      │ (None, 1)              │       665,089 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,154,913 (8.22 MB)
 Trainable params: 1,488,088 (5.68 MB)
 Non-trainable params: 666,825 (2.54 MB)
Код формирования модели НС:
from keras.layers import Input, Dense
from keras.layers import BatchNormalization, LeakyReLU
from keras.models import Model
from keras.optimizers import Adam
img_rows = 28
img_cols = 28
img_shape = (img_rows * img_cols,)
latent_dim = 100
loss_d = 'binary_crossentropy'
loss_c = 'binary_crossentropy'
optimizer_d = Adam(0.0002, 0.5)
optimizer_c = Adam(0.0002, 0.5)
def one_g_dense(units, x):
    x = BatchNormalization(momentum = 0.8)(x)
    x = Dense(units)(x)
    return LeakyReLU(alpha = 0.2)(x)
def one_d_dense(units, x):
    x = Dense(units)(x)
    return LeakyReLU(alpha = 0.2)(x)
# Генератор
def build_generator():
    inp = Input(shape = (latent_dim,))
    x = one_g_dense(256, inp)
    x = one_g_dense(512, x)
    x = one_g_dense(1024, x)
    x = Dense(784, activation = 'tanh')(x)
    generator = Model(inp, x, name = 'Generator')
    generator.summary()
    return generator
# Дискриминатор
def build_discriminator():
    img = Input(shape = img_shape)
    x = one_d_dense(512, img)
    x = one_d_dense(256, x)
    x = one_d_dense(128, x)
    output = Dense(1, activation = 'sigmoid')(x)
    discriminator = Model(img, output, name = 'Discriminator')
    discriminator.summary()
    discriminator.compile(loss = loss_d, optimizer = optimizer_d, metrics = ['accuracy'])
    return discriminator
# Обобщенная модель
def build_combined(generator, discriminator):
    # Генератор принимает шум и возвращает (генерирует) изображения
    inp = Input(shape = (latent_dim,))
    img = generator(inp)
    # Дискриминатор принимает сгенерированное изображение
    # и классифицирует его либо как истинное, либо как поддельное, возвращая validity
    output = discriminator(img)
    # Объединенная модель
    combined = Model(inp, output, name = 'Combined')
    # Поскольку метрика не задана, то после каждой эпохи вычисляются только потери
    combined.compile(loss = loss_c, optimizer = optimizer_c)
    combined.summary()
    return combined
generator = build_generator() # Построение генератора
discriminator = build_discriminator() # Построение и компиляция дискриминатора
combined = build_combined(generator, discriminator)

Загрузка ОМ.
x_trn = load_train()
x_trn = 2.0 * x_trn - 1.0 # Приводим к диапазону [-1, 1] (gen_activation = 'tanh')

Обучение модели.
steps = 30001
batch_size = 32
sample_interval = 3000
train(discriminator, generator, combined)
def train(discriminator, generator, combined):
    # Метки истинных изображений (поддельных: valid - 1)
    valid = np.ones((batch_size, 1))
    d_loss, d_acc, g_loss = [], [], []
    for step in range(steps):
        print(step)
        # Обучаем дискриминатор
        # Выбираем batch_size случайных изображений из обучающего множества
        idx = np.random.randint(0, x_trn.shape[0], batch_size)
        imgs = x_trn[idx]
        # Шум, подаваемый на вход генератора
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        # Генерируем batch_size изображений (поддельных)
        gen_imgs = generator.predict(noise)
        # Обучаем дискриминатор, подавая ему сначала настоящие, а затем поддельные изображения
        discriminator.trainable = True
        d_hist_real = discriminator.train_on_batch(imgs, valid)
        d_hist_fake = discriminator.train_on_batch(gen_imgs, valid - 1)
        # Усредняем результаты и получаем средние потери и точность
        d_hist = 0.5 * np.add(d_hist_real, d_hist_fake)
        # Обучение обобщенной модели. Реально обучается только генератор
        discriminator.trainable = False
        noise = np.random.normal(0, 1, (batch_size, latent_dim))
        # Обучение генератора. Метки изображений valid (единицы),
        # то есть изображения, порожденные генератором при его обучении, считаются истинными
        g_ls = combined.train_on_batch(noise, valid)
        if step % 100 == 0:
            d_loss.append(d_hist[0])
            d_acc.append(d_hist[1])
            g_loss.append(g_ls)
        # Потери и точность дискриминатора и потери генератора
        if step % (sample_interval / 10) == 0:
            print("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (step, d_hist[0], 100 * d_hist[1], g_ls))
        # Генерируем и сохраняем рисунок с 25-ю изображениями
        if step % sample_interval == 0:
            save_sample_images(latent_dim, generator, step)
    # Сохраняем обученный генератор в файл
    file_gen = pathToHistory + 'generator_model_%03d.keras' % steps
    generator.save(file_gen)
    print('Модель генератора сохранена в файл', file_gen)
    #
    # Вывод историй обучения в файлы
    fn_d_loss, fn_d_acc, fn_g_loss = 'd_loss.txt', 'd_acc.txt', 'g_loss.txt'
    print('Истории сохранены в файлы:\n' + fn_d_loss + '\n' + fn_d_acc + '\n' + fn_g_loss)
    print('Путь:', pathToHistory)
    with open(pathToHistory + fn_d_loss, 'w') as output:
        for val in d_loss: output.write(str(val) + '\n')
    with open(pathToHistory + fn_d_acc, 'w') as output:
        for val in d_acc: output.write(str(val) + '\n')
    with open(pathToHistory + fn_g_loss, 'w') as output:
        for val in g_loss: output.write(str(val) + '\n')
    # Вывод графиков историй обучения
    yMax = max(g_loss)
    cnt = len(g_loss)
    rng = np.arange(cnt)
    fig, ax = plt.subplots(figsize = (7, 4))
    ax.scatter(rng, d_loss, marker = 'o', c = 'blue', edgecolor = 'black')
    ax.scatter(rng, g_loss, marker = 'x', c = 'red')
    ax.set_title('Потери генератора (x) и дискриминатора (o)')
    ax.set_ylabel('Потери')
    ax.set_xlabel('Эпоха / 100')
    ax.set_xlim([-0.5, cnt])
    ax.set_ylim([0, 1.1 * yMax])
    fig.show()
Сохранение результатов
def save_sample_images(latent_dim, generator, step):
    r, c = 5, 5 # Выводим и сохраняем 25 изображений
    # latent_dim - размер шума, подаваемого на вход генератора
    noise = np.random.normal(0, 1, (r * c, latent_dim))
    gen_imgs = generator.predict(noise)
    # Возвращаемся к диапазону [0, 1]
    gen_imgs = 0.5 * gen_imgs + 0.5
    gen_imgs = gen_imgs.reshape(-1, 28, 28)
    fig, axs = plt.subplots(r, c)
    cnt = 0
    for i in range(r):
        for j in range(c):
            axs[i, j].imshow(gen_imgs[cnt], cmap = 'gray')
            axs[i, j].axis('off')
            cnt += 1
    fig.savefig(pathToHistory + '%d.png' % step)
    plt.close()
Возможный результат и графики потерь см. на рис.

VII. cGAN

Обучается на MNIST.
В отличие от GAN генерирует изображения заданного класса.
Класс задается меткой, которая поступает на входы генератора и дискриминатора.
Далее метка преобразуется в вектор (слой Embedding).
Структуры моделей см. на рис.

Общие данные. Входные слои и вход генератора.
x_trn, y_trn = load_data('train')
x_trn = 2.0 * x_trn - 1.0 # Приводим к диапазону [-1, 1] (gen_activation = 'tanh')
num_classes = 10
latent_dim = 100
noise = Input((latent_dim,))
label = Input((1,)) # Метка класса
inp_c = [noise, label] # Вход генератора и обобщенной модели
optimizer_d = Adam(0.0002, 0.5)
optimizer_c = Adam(0.0002, 0.5)
n_steps = 30001 # Число шагов обучения (60001)
batch_size = 32 # Размер пакета обучения (число генерируемых изображений)
sample_interval = 3000 # Интервал между сохранением сгенерированных изображений в файл
file_gen = 'gen_model_%03d.keras' % n_steps

Отображение скаляра в вектор.
from keras.layers import multiply, Embedding, Flatten
# Формирование входа генератора и дискриминатора.
def one_emb(num_classes, emb_dim, inp_g_d, label):
    lbl = Embedding(num_classes, emb_dim)(label)
    lbl = Flatten()(lbl)
    inp_gen_dis = multiply([inp_g_d, lbl])
    return inp_gen_dis
# Генератор
def build_generator():
    inp_gen = one_emb(num_classes, latent_dim, noise, label)
    x = one_g_dense(inp_gen, 256)
    x = one_g_dense(x, 512)
    x = one_g_dense(x, 1024)
    out = Dense(784, activation = 'tanh')(x)
    generator = Model(inp_c, out, name = 'Generator')
    return generator
# Дискриминатор
def build_discriminator():
    img = Input((784,))
    inp_dis = one_emb(num_classes, 784, img, label)
    x = one_d_dense(inp_dis, 512)
    x = one_d_dense(x, 256)
    x = one_d_dense(x, 128)
    out = Dense(1, activation = 'sigmoid')(x)
    discriminator = Model([img, label], out, name = 'Discriminator')
    discriminator.compile(loss = 'binary_crossentropy', optimizer = optimizer_d, metrics = ['accuracy'])
    return discriminator
# cGAN
def build_combined():
    gen_img = generator(inp_c)
    inp_d = [gen_img, label]
    out = discriminator(inp_d)
    combined = Model(inp_c, out, name = 'combined')
    combined.compile(loss = 'binary_crossentropy', optimizer = optimizer_c)
    return combined
# Входные данные дискриминатора и генератора
def m_inp(N):
    idx = np.random.randint(0, x_trn.shape[0], N)
    d_n = x_trn[idx]
    l_n = y_trn[idx] # Метки изображений
    g_n_noise = np.random.normal(0, 1, (N, latent_dim))
    return d_n, l_n, g_n_noise
# Обучение
def train():
    # Метки истинных изображений (ложных: valid - 1)
    valid = np.ones(bath_size)
    for step in range(n_steps):
        # Данные для формирования входов дискриминатора и генератора
        d_n, l_n, g_n_noise = m_inp(batch_size)
        inp_gen = [g_n_noise, l_n] # Шум и метки
        inp_d = [d_n, l_n] # Истинные данные и метки
        # Генерируем batch_size изображений
        gen_imgs = generator.predict(inp_gen)
        inp_d2 = [gen_imgs, l_n] # Сгенерированные данные и метки
        # Обучаем дискриминатор
        discriminator.trainable = True
        d_hist_real = discriminator.train_on_batch(inp_d, valid)
        d_hist_fake = discriminator.train_on_batch(inp_d2, valid - 1)
        # Усредняем результаты и получаем средние потери и точность
        d_ls, d_a = 0.5 * np.add(d_hist_real, d_hist_fake)
        # Обучение обобщенной модели. Реально обучается только генератор
        sampled_labels = np.random.randint(0, num_classes, bath_size)
        inp_gen = [g_n_noise, sampled_labels]
        discriminator.trainable = False
        g_ls = combined.train_on_batch(inp_gen, valid)
        # Потери и точность дискриминатора и генератора
        if step % (sample_interval / 10) == 0:
            print("%d [D loss: %f, acc: %.2f%%] [G loss: %f]" % (step, d_ls, 100 * d_a, g_ls))
        if step % sample_interval == 0: # Генерируем и сохраняем рисунок
            save_sample_images()

# Формирование, обучение и сохранение модели
generator = build_generator()
discriminator = build_discriminator() # Построение и компиляция дискриминатора
combined = build_combined() # Обобщенная модель
train()
generator.compile() # Сохраняем обученный генератор в файл
generator.save(file_gen)

Загрузка проверочных данных, модели, прогноз
from keras.models import load_model 
x_tst, y_tst = load_data('test')
x_tst = 2.0 * x_tst - 1.0 # Приводим к диапазону [-1, 1] (gen_activation = 'tanh')
generator = load_model(file_gen)
N = 20
d_n, l_n, g_n_noise = m_inp(N)
gen_imgs = generator.predict([g_n_noise, l_n])
d_n = 0.5 * d_n + 0.5 # Возвращаемся к диапазону [0, 1]
gen_imgs = 0.5 * gen_imgs + 0.5
fig, axs = plt.subplots(2, N, figsize = (12, 2.5))
fig.suptitle('Прогноз на проверочных данных')
for i in range(N):
    axs[0, i].title.set_text(l_n[i])
    axs[0, i].imshow(d_n[i].reshape(28, 28), cmap = 'gray')
    axs[1, i].title.set_text(l_n[i])
    axs[1, i].imshow(gen_imgs[i].reshape(28, 28), cmap = 'gray')
    for k in range(2): axs[k, i].axis('off')
plt.show()

VIII. Варианты GAN
См. doc-файл.

IX. Функции определения сходства/различия изображений
import numpy as np
from skimage.metrics import mean_squared_error
import cv2, scipy
from skimage.metrics import structural_similarity as ssim
from scipy.spatial.distance import euclidean as e_dist
#
sz = 784 * 3
image1 = np.arange(sz).reshape(28, 28, 3) / sz
image2 = np.arange(sz).reshape(28, 28, 3) / sz + 0.1
#
# Средняя квадратическая ошибка
mse = ((image1 - image2) ** 2).mean()
mse2 = mean_squared_error(image1, image2)
# Пиковое отношение сигнала к шуму
# https://habr.com/ru/articles/126848/
psnr = cv2.PSNR(image1, image2, mse)
# Индекс структурного сходства
# https://habr.com/ru/articles/126848/
# https://scikit-image.org/docs/stable/auto_examples/transform/plot_ssim.html
score_ssim, diff = ssim(image1, image2, full = True, multichannel = True)
или (в зависимости от версии)
score_ssim, diff = ssim(image1, image2, full = True, channel_axis = 2)
score_ssim2, diff = ssim(image1.flatten(), image2.flatten(), full = True)
# Евклидово расстояние
ec_dst = np.linalg.norm(image1 - image2)
ec_dst2 = e_dist(image1.flatten(), image2.flatten())
# Косинусное расстояние между векторами
# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html
cs_dst = scipy.spatial.distance.cosine(image1.flatten(), image2.flatten())

print('mse =', mse)
print('mse2 =', mse2)
print('psnr =', psnr)
print('ssim =', score_ssim)
print('ssim2 =', score_ssim2)
print('ec_dst =', ec_dst)
print('ec_dst2 =', ec_dst2)
print('cs_dst =', cs_dst)
mse = 0.009999999999999998
mse2 = 0.009999999999999998
psnr = -19.99999999999996
ssim = 0.9676383577979917
ssim2 = 0.9281674094684876
ec_dst = 4.8497422611928585
ec_dst2 = 4.849742261192823
cs_dst = 0.002826967506080802

X. Метод Монте-Карло
 Позволяет с некоторой ошибкой решить задачу в результате проведении 
 достаточно большого количества вычислительных экспериментов.
 Например, число "пи" можно найти следующим образом:
 1. Задать квадрат со стороной 2R. Поместить центр квадрата в начало координат.
    Стороны квадрата параллельны осям координат.
 2. N = 1000 - число экспериментов.
 3. n = 0 - число случайно сгенерированных точек, попавших во вписанный в квадрат круг радиуса R.
 4. Выполнить N раз:
  4.1. Генерируем случайную точку с координатами x, y: -R <= x <= R, -R <= y <= R.
  4.2. Если x^2 + y^2 <= R^2:
         n +=1
 5. пи = 4 * n / N.

import numpy as np
r = 10
r2 = r**2
N = 500000
n = 0
for k in range(N):
    p = np.random.uniform(-r, r, 2)
    if p[0]**2 + p[1]**2 <= r2: n += 1
print('пи =', 4 * n / N)


XI. Контрольные вопросы и задача.
 - контрольные вопросы - материалам лекции;
 - задача на функции определения сходства/различия изображений и метод Монте-Карло.
   Задать вид множества MNIST (обучающее или проверочное).
   Найти в выбранном множестве два наиболее похожих изображения цифр 1 и 7,
   используя поочередно следующие метрики:
    - средняя квадратическая ошибка; MeanSquaredError class
    - евклидово расстояние;
    – косинусное расстояние;
    - индекс структурного сходства.
   Для каждой метрики вывести расстояние, индексы и рисунки найденных изображений.
   Для сокращения объема вычислений использовать метод Монте-Карло.

XII. Задачи 11 и 12

Решения предоставляются в виде отчетов.

ЛР 11. НС со сверточными слоями.
Число эпох - не менее 20.
Приводятся сравнительные результаты для моделей НС, содержащих в том числе следующие слои:
- сверточные, пакетной нормализации и подвыборки;
- сверточные и пакетной нормализации;
- сверточные и подвыборки;
- сверточные.
В отчете приводятся:
- model.summary();
- графики обучения;
- classification_report;
- таблица со сравнительными результатами обучения, содержащая в том числе:
  - число обучаемых параметров;
  - время обучения.
Обученные модели сохраняются в файлы.

ЛР 12. cGAN с MNIST.
В отчете приводятся:
- model.summary();
- функции активации и потерь;
- графики обучения;
- примеры работы обученной модели:
   - изображения из ОМ с метками, поступившими на вход генератора;
   - соответствующие им сгенерированные изображения.
Обученная модель (генератор) сохраняется в файл,
который используется при демонстрации работы генератора.

